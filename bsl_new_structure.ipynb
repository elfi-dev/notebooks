{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5137c76",
   "metadata": {},
   "source": [
    "This tutorial is generated from a [Jupyter](http://jupyter.org/) notebook that can be found [here](https://github.com/elfi-dev/notebooks). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81fc830f",
   "metadata": {},
   "source": [
    "## BSL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "051e783b",
   "metadata": {},
   "source": [
    "[Bayesian synthethic likelihood](https://doi.org/10.1080/10618600.2017.1302882) (BSL) methods sample an approximate posterior distribution based on comparison between the observed data and a [synthetic likelihood](https://www.nature.com/articles/nature09319) estimated based on repeated simulations. \n",
    "\n",
    "This tutorial demonstrates how to use various BSL methods for LFI in ELFI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15036d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import elfi\n",
    "from elfi.methods.bsl import pre_sample_methods, pdf_methods\n",
    "\n",
    "seed = 1\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f63b629c",
   "metadata": {},
   "source": [
    "We use the MA(2) model introduced in the [ELFI tutorial](https://elfi.readthedocs.io/en/latest/usage/tutorial.html#inference-with-elfi-case-ma-2-model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830d07e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from elfi.examples import ma2\n",
    "m = ma2.get_model(n_obs=50, true_params=[0.6, 0.2], seed_obs=seed)\n",
    "elfi.draw(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ecbdd02",
   "metadata": {},
   "source": [
    "### Tutorial example\n",
    "\n",
    "BSL in ELFI proceeds so that we first define how the observed and simulated data are converted to approximate likelihood scores and then sample the approximate posterior distribution estimated based on the likelihood scores. Here we demonstrate the process with standard BSL.\n",
    "\n",
    "1. Likelihood calculation\n",
    "\n",
    "BSL compares features calculated based observed and simulated data to estimate the likelihood that the same parameter values produced the observed and simulated data. \n",
    "In practice the approximate likelihood that certain parameters `params` produced the observed data is calculated based on the observed features and a distribution model that describes the feature distribution at `params`. The model is estimated based on `nsim_round` features simulated with `params`.\n",
    "\n",
    "- Features\n",
    "\n",
    "Features used in likelihood calculation must be included as observable nodes in the ELFI model. Observable nodes in our example include the simulator node `MA2` and the summaries `S1` and `S2`. To match examples used in the BSL literature, we do not use summaries but estimate likelihood scores based on the observed and simulated data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c12fe68",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = 'MA2'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "758da25b",
   "metadata": {},
   "source": [
    "As the standard likelihood calculation method models the simulated feature distribution as multivariate normal, it is useful to visualise feature distributions to assess normality. Here we choose some parameter values `params` that we believe could reproduce the observed data and visualise the feature distribution over `nsim` repeated simulations. If the feature distributions deviate from normal, a semiparametric likelihood calculation method could be more appropriate than the standard likelihood calculation method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409be69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'t1': 0.6, 't2': 0.2}\n",
    "nsim = 10000\n",
    "pre_sample_methods.plot_features(m, params, nsim, feature_names, seed=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55cfe979",
   "metadata": {},
   "source": [
    "- Likelihood calculation method\n",
    "\n",
    "The likelihood calculation methods available in ELFI include `standard_likelihood`, `unbiased_likelihood`, `semiparametric_likelihood`, and `robust_likelihood`. We can either choose a method from this list or provide a custom method that takes the observed and simulated data as input and returns an appropriate likelihood score.\n",
    "\n",
    "Here we choose the standard synthetic likelihood calculation method. This method approximates the simulated feature distribution with a multivariate normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f6acb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "likelihood = pdf_methods.standard_likelihood()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4ce905",
   "metadata": {},
   "source": [
    "- Simulation count\n",
    "\n",
    "BSL methods are most computationally efficient when `nsim_round` is selected so that the standard deviation between log-likelihood scores is between 1 and 2. Here we calculate the standard devitation between `M` likelihood scores calculated based on `nsim` features simulated with `params`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e138c7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'t1': 0.6, 't2': 0.2}\n",
    "nsim = [100, 300, 500]\n",
    "std_value = pre_sample_methods.log_SL_stdev(m, params, nsim, feature_names, likelihood=likelihood, M=100, seed=seed)\n",
    "std_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827fff1f",
   "metadata": {},
   "source": [
    "The simulation count that is needed to reduce the standard deviation to the recommended level depends on the feature dimension, and we see that with the selected features, around 500 simulations are needed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b36f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "nsim_round = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba2a4a57",
   "metadata": {},
   "source": [
    "2. BSL sampler\n",
    "\n",
    "With `feature_names`, `likelihood`, and `nsim_round`, we have decided how the likelihood scores are calculated and can create a `BSL` instance to sample the approximate posterior distribution estimated based on the selected likelihood scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c51712",
   "metadata": {},
   "outputs": [],
   "source": [
    "standard_bsl = elfi.BSL(m, nsim_round, feature_names=feature_names, likelihood=likelihood, seed=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a918a96",
   "metadata": {},
   "source": [
    "We can see here that `feature_names` and `likelihood` are optional parameters. `feature_names` defaults to all summary nodes in the model `m` and `likelihood` defaults to the standard synthetic likelihood calculation method. Other notable parameters include `batch_size` which defaults to the simulation count `nsim_round`. \n",
    "\n",
    "To proceed, we call the `sample` method to set sampler parameters and sample a chain from the approximate posterior distribution. BSL uses MCMC sampling with a Metropolis-Hastings step. Required parameters are the sample size and a covariance matrix for the proposal distribution. Here we also set the initial parameter values `params0` to the true parameter values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053a3413",
   "metadata": {},
   "outputs": [],
   "source": [
    "mcmc_iterations = 2000  # sample size\n",
    "est_post_cov = np.array([[0.02, 0.01], [0.01, 0.02]])  # covariance matrix for the proposal distribution\n",
    "params0 = [0.6, 0.2]\n",
    "res = standard_bsl.sample(mcmc_iterations, est_post_cov, params0=params0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d94e167",
   "metadata": {},
   "outputs": [],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e48b2e7c",
   "metadata": {},
   "source": [
    "As MCMC draws correlated samples, it is useful to estimate the effective sample size (ESS) as an estimate of the equivalent number of independent iterations that the MCMC chain represents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a27355",
   "metadata": {},
   "outputs": [],
   "source": [
    "res.compute_ess()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c0364e3",
   "metadata": {},
   "source": [
    "We can also visualise the chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196e6ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "res.plot_traces();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04207ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "mbins = 15\n",
    "res.plot_marginals(reference_value={'t1': 0.6, 't2': 0.2}, bins=mbins);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6564a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "res.plot_pairs(reference_value={'t1': 0.6, 't2': 0.2}, bins=mbins);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13380804",
   "metadata": {},
   "source": [
    "This concludes the standard BSL example. The next sections first discuss how to reduce the simulation count `nsim_round` and then introduce alternative likelihood calculation methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9397419",
   "metadata": {},
   "source": [
    "### Covariance matrix estimation\n",
    "\n",
    "We learned that synthetic likelihood calculation uses `nsim_round` simulations to estimate the feature distribution at all proposed parameter values. The simulation count needed to calculate reliable estimates for the feature distribution mean and covariance increases with feature dimension. The covariance matrix estimate in particular becomes unstable if `nsim_round` is too small. However we can reduce the required simulation count with penalised covariance estimation methods.\n",
    "\n",
    "The covariance estimation in `standard_likelihood` and `semiparametric_likelihood` is controlled with `shrinkage` and `penalty` options. `shrinkage` selects the penalised covariance matrix estimation method (`'warton'` or `'glasso'`) and `penalty` controls a parameter used in estimation.  The examples in this section use `standard_likelihood`.\n",
    "\n",
    "Let us visualise an example covariance matrix estimated based on `nsim` simulations at `params`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc038913",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'t1': 0.6, 't2': 0.2}\n",
    "nsim = 10000\n",
    "pre_sample_methods.plot_covariance_matrix(m, params, nsim, feature_names, seed=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "362582f7",
   "metadata": {},
   "source": [
    "Since consecutive features are correlated and have non-zero between-feature covariances, we would loose information with `shrinkage='warton'` which biases all between-feature covariances towards zero. However as most between-feature covariances are close to zero, a sparse estimate that retains some covariance structure could be appropriate.  Hence we choose `shrinkage='glasso'` which uses [graphical lasso](https://scikit-learn.org/stable/modules/covariance.html#sparse-inverse-covariance) to estimate a sparse inverse covariance matrix.\n",
    "\n",
    "In addition to the likelihood calculation and covariance estimation methods, we must choose the `penalty` parameter used in covariance estimation. We can use `select_penalty` method to find parameter values that result in log-likelihood scores with standard deviation closest to a reference value `sigma` for selected simulation counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d056679",
   "metadata": {},
   "outputs": [],
   "source": [
    "likelihood = pdf_methods.standard_likelihood()\n",
    "shrinkage = 'glasso'\n",
    "sigma = 1.5\n",
    "nsim = [100, 300, 500]\n",
    "params = {'t1': 0.6, 't2': 0.2}\n",
    "penalty, std_value = pre_sample_methods.select_penalty(m, nsim, params, feature_names, sigma=sigma, shrinkage=shrinkage, likelihood=likelihood, seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5266e514",
   "metadata": {},
   "outputs": [],
   "source": [
    "std_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032f8a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "penalty"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a85d944",
   "metadata": {},
   "source": [
    "Here we reach the same standard deviation at all simulation counts as the recommended penalty increases to compensate for the decrease in simulation count. However, as we do not want to distort the estimated covariance matrix, we should choose a simulation count that works with a reasonably small penalty value.\n",
    "\n",
    "Let us update the likelihood calculation method and simulation count:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be49fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "likelihood = pdf_methods.standard_likelihood(shrinkage='glasso', penalty=penalty[1])\n",
    "nsim_round = nsim[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6baa604a",
   "metadata": {},
   "source": [
    "This method is known as [BSLasso](https://doi.org/10.1080/10618600.2018.1537928)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21017432",
   "metadata": {},
   "outputs": [],
   "source": [
    "bslasso = elfi.BSL(m, nsim_round, feature_names=feature_names, likelihood=likelihood, seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16dead53",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = bslasso.sample(mcmc_iterations, est_post_cov, params0=params0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0cc6072",
   "metadata": {},
   "outputs": [],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4c4c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "res.plot_pairs(reference_value={'t1': 0.6, 't2': 0.2}, bins=mbins);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e867caa",
   "metadata": {},
   "source": [
    "We observed above that the features used in this example have a covariance structure that can be approximated with a sparse covariance matrix. If the covariance structure is expected to be constant with respect to the simulator parameters, we can also estimate a whitening transformation to decorrelate the features prior to covariance matrix estimation. This should remove the between-feature covariances and allow for more reduction in the simulation count. \n",
    "\n",
    "We estimate the transformation matrix based on `nsim` features simulated with `params` that we believe could reproduce the observed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ededaae",
   "metadata": {},
   "outputs": [],
   "source": [
    "likelihood_type = 'standard'\n",
    "nsim = 20000\n",
    "params = {'t1': 0.6, 't2': 0.2}\n",
    "W = pre_sample_methods.estimate_whitening_matrix(m, nsim, params, feature_names, likelihood_type=likelihood_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe90736",
   "metadata": {},
   "source": [
    "When a whitening transformation is used, `shrinkage='warton'` is recommended as the penalised covariance estimation method. This accepts `penalty` between 0 and 1. Since we expect to have decorrelated features, we can choose for example `penalty=0.8`, or we can use `select_penalty` to find suitable values as in the previous example. In case we use `select_penalty`, the whitening transformation is included as an additional input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f1a2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "likelihood = pdf_methods.standard_likelihood()\n",
    "shrinkage = 'warton'\n",
    "sigma = 1.5\n",
    "nsim = [100, 300]\n",
    "params = {'t1': 0.6, 't2': 0.2}\n",
    "penalty, std_value = pre_sample_methods.select_penalty(m, nsim, params, feature_names, whitening=W, sigma=sigma, shrinkage=shrinkage, likelihood=likelihood, seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a5d206",
   "metadata": {},
   "outputs": [],
   "source": [
    "std_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f13b2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "penalty"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe3d6c50",
   "metadata": {},
   "source": [
    "Let us update the likelihood calculation method and simulation count:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6f99d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "likelihood = pdf_methods.standard_likelihood(whitening=W, shrinkage=shrinkage, penalty=penalty[0])\n",
    "nsim_round = nsim[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1191fc5b",
   "metadata": {},
   "source": [
    "This method is known as [wBSL](https://doi.org/10.1080/10618600.2021.1979012)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f81521",
   "metadata": {},
   "outputs": [],
   "source": [
    "wbsl = elfi.BSL(m, nsim_round, feature_names=feature_names, likelihood=likelihood, seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a012ff97",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = wbsl.sample(mcmc_iterations, est_post_cov, params0=params0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b2a446",
   "metadata": {},
   "outputs": [],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11998556",
   "metadata": {},
   "outputs": [],
   "source": [
    "res.plot_pairs(reference_value={'t1': 0.6, 't2': 0.2}, bins=mbins);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a3b4de",
   "metadata": {},
   "source": [
    "### Robust likelihood calculation\n",
    "\n",
    "The previous examples all used `standard_likelihood` which models the simulated feature distribution as multivariate normal. Alternatives include the `semiparametric_likelihood` that relaxes the normality assumption and `robust_likelihood` can be used to compensate for model misspecification.\n",
    "\n",
    "Let us start with `semiparametric_likelihood`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07dcdbe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "likelihood = pdf_methods.semiparametric_likelihood()\n",
    "nsim_round = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eab9de2",
   "metadata": {},
   "source": [
    "BSL with semi-parametric synthetic likelihood calculation is known as [semiBSL](https://link.springer.com/article/10.1007/s11222-019-09904-x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80eccf01",
   "metadata": {},
   "outputs": [],
   "source": [
    "semibsl = elfi.BSL(m, nsim_round, feature_names=feature_names, likelihood=likelihood, seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171223e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = semibsl.sample(mcmc_iterations, est_post_cov, params0=params0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51722d07",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5743c46d",
   "metadata": {},
   "outputs": [],
   "source": [
    "res.plot_pairs(reference_value={'t1': 0.6, 't2': 0.2}, bins=mbins);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08bf5f76",
   "metadata": {},
   "source": [
    "Semi-parametric likelihood calculation can also be combined with a whitening transformation and penalised covariance matrix estimation. This version is known as [wsemiBSL](https://doi.org/10.48550/arXiv.2007.01485).\n",
    "\n",
    "Finally the last methods demonstrated in this notebook use `robust_likelihood` to perform parameter inference even when the simulator model is unable to replicate the observed features. These are known as [robust BSL](https://doi.org/10.1080/10618600.2021.1875839) (R-BSL) methods. In practice the robust likelihood calculation method has free parameters that adjust either the sample mean or sample covariance to compensate for model misspecification, and the R-BSL methods sample both the simulator parameters and the free parameters `gamma`.\n",
    "\n",
    "\n",
    "We run the last examples with summarised data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0690bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = ['S1', 'S2']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af7c043d",
   "metadata": {},
   "source": [
    "We can choose either `robust_likelihood('mean')` or `robust_likelihood('variance')`. We start with mean adjustment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b604f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "likelihood = pdf_methods.robust_likelihood('mean')\n",
    "nsim_round = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9e6d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "rbsl_m = elfi.BSL(m, nsim_round, feature_names=feature_names, likelihood=likelihood, seed=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b31da7",
   "metadata": {},
   "source": [
    "R-BSL methods use a slice sampler for the `gamma` parameters, and the `sample` method is now used to set both the MCMC and slice sampler parameters. The slice sampler parameters include step size `w`, maximum iteration count `max_iter`, and a parameter `tau` that scales the prior distribution used for `gamma`. Here we use the default values for `w` and `max_iter` but set the prior distribution scale `tau` to a small value to indicate that we do not believe that the model is misspecified.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304853fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = rbsl_m.sample(mcmc_iterations, est_post_cov, params0=params0, tau=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983081b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dcb1e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "res.plot_pairs(reference_value={'t1': 0.6, 't2': 0.2}, bins=mbins);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c428f5a0",
   "metadata": {},
   "source": [
    "We can also visualise the sampled `gamma` parameters to see how the sample mean was adjusted in the accepted sample. Each feature is adjusted separately so the `gamma` parameter dimension matches the selected feature dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55fecf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma_dict = dict(zip(['gamma_{}'.format(index) for index in range(rbsl_m.observed.size)], np.transpose(res.outputs['gamma'])))\n",
    "elfi.visualization.visualization.plot_marginals(gamma_dict, bins=mbins);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674a7ef5",
   "metadata": {},
   "source": [
    "R-BSL with variance adjustment follows the same process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a2f045",
   "metadata": {},
   "outputs": [],
   "source": [
    "likelihood = pdf_methods.robust_likelihood('variance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729999f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "rbsl_v = elfi.BSL(m, nsim_round, feature_names=feature_names, likelihood=likelihood, seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c36af41",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = rbsl_v.sample(mcmc_iterations, est_post_cov, params0=params0, tau=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56159083",
   "metadata": {},
   "outputs": [],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2172810d",
   "metadata": {},
   "outputs": [],
   "source": [
    "res.plot_pairs(reference_value={'t1': 0.6, 't2': 0.2}, bins=mbins);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71482d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma_dict = dict(zip(['gamma_{}'.format(index) for index in range(rbsl_v.observed.size)], np.transpose(res.outputs['gamma'])))\n",
    "elfi.visualization.visualization.plot_marginals(gamma_dict, bins=mbins);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70203a4a",
   "metadata": {},
   "source": [
    "### Parallelising simulations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722cef84",
   "metadata": {},
   "source": [
    "To parallelise the simulated feature generation in BSL, we can choose a batch size that is smaller than the simulation count and let an appropriate ELFI client run batches in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad01e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "elfi.set_client('multiprocessing')\n",
    "bsl = elfi.BSL(m, 200, batch_size=50, seed=seed)\n",
    "res = bsl.sample(mcmc_iterations, est_post_cov, bar=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (elfi_dev)",
   "language": "python",
   "name": "elfi_dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
